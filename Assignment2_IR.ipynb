{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from math import log, sqrt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources if not already available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocess function with stopword removal\n",
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove punctuation and stopwords\n",
        "    return tokens\n",
        "\n",
        "def read_documents(zip_path):\n",
        "    documents = {}\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        for filename in z.namelist():\n",
        "            with z.open(filename) as f:\n",
        "                documents[filename] = preprocess(f.read().decode('utf-8'))\n",
        "    return documents\n",
        "\n",
        "def compute_tf_idf(query, documents):\n",
        "    N = len(documents)\n",
        "    df = defaultdict(int)\n",
        "\n",
        "    # Calculate df for each term in documents\n",
        "    for doc in documents.values():\n",
        "        unique_terms = set(doc)\n",
        "        for term in unique_terms:\n",
        "            df[term] += 1\n",
        "\n",
        "    # Compute TF-IDF for the query\n",
        "    query_weights = {}\n",
        "    for term in query:\n",
        "        tf = 1 + log(query.count(term)) if query.count(term) > 0 else 0\n",
        "        idf = log(N / df[term]) if df[term] > 0 else 0\n",
        "        query_weights[term] = tf * idf\n",
        "\n",
        "    # Normalize the query vector\n",
        "    query_magnitude = sqrt(sum(weight ** 2 for weight in query_weights.values()))\n",
        "    query_weights_normalized = {term: weight / query_magnitude for term, weight in query_weights.items() if weight > 0}\n",
        "\n",
        "    # Calculate TF for each document and cosine similarity\n",
        "    scores = []\n",
        "    for doc_name, doc in documents.items():\n",
        "        doc_tf = defaultdict(float)\n",
        "        for term in doc:\n",
        "            tf = 1 + log(doc.count(term)) if doc.count(term) > 0 else 0\n",
        "            doc_tf[term] += tf\n",
        "\n",
        "        # Normalize document\n",
        "        doc_magnitude = sqrt(sum(weight ** 2 for weight in doc_tf.values()))\n",
        "        if doc_magnitude > 0:\n",
        "            normalized_doc_tf = {term: weight / doc_magnitude for term, weight in doc_tf.items()}\n",
        "        else:\n",
        "            normalized_doc_tf = {}\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarity = sum(query_weights_normalized.get(term, 0) * normalized_doc_tf.get(term, 0) for term in query_weights_normalized)\n",
        "        scores.append((doc_name, similarity))\n",
        "\n",
        "    # Rank documents based on scores\n",
        "    ranked_documents = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    return ranked_documents[:10]  # Top 10 documents\n",
        "\n",
        "\n",
        "zip_path = 'Corpus.zip'\n",
        "documents = read_documents(zip_path)\n",
        "\n",
        "# User query input\n",
        "user_query = input(\"Enter your query: \")\n",
        "query_tokens = preprocess(user_query)\n",
        "\n",
        "# Compute scores and get top documents\n",
        "top_documents = compute_tf_idf(query_tokens, documents)\n",
        "\n",
        "# Print results\n",
        "print(\"Top 10 documents:\")\n",
        "for doc, score in top_documents:\n",
        "    print(f\"{doc}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgtSdmy3hGHN",
        "outputId": "c29dfdc0-fa1c-4ceb-f7fe-3cc26eeb61df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: ‘Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
            "Top 10 documents:\n",
            "zomato.txt: 0.5903\n",
            "swiggy.txt: 0.1695\n",
            "instagram.txt: 0.1083\n",
            "flipkart.txt: 0.0516\n",
            "messenger.txt: 0.0460\n",
            "Amazon.txt: 0.0330\n",
            "reddit.txt: 0.0328\n",
            "paypal.txt: 0.0266\n",
            "Discord.txt: 0.0240\n",
            "nike.txt: 0.0239\n"
          ]
        }
      ]
    }
  ]
}